{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d859be8",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5082b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, EarlyStoppingCallback, pipeline\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebd3fe",
   "metadata": {},
   "source": [
    "#### Check for GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c3d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise SystemExit(\"GPU (CUDA) not available. Aborting training process.\")\n",
    "\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f738ba",
   "metadata": {},
   "source": [
    "#### Load models and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d69511",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"ProsusAI/finbert\"\n",
    "finetuned_model_name = \"./models/finbert-finetuned\"\n",
    "dataset_name = \"modestus/bitcoin_sentiment_analysis\"\n",
    "summarizer_name = \"facebook/bart-large-cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1},\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summarizer model and tokenizer\n",
    "summarizer = pipeline(\"summarization\", model=summarizer_name, device=0)\n",
    "summarizer_tokenizer = summarizer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7da509ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa89bf",
   "metadata": {},
   "source": [
    "#### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d1dd4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize text and format as \"title - summary\"\n",
    "def summarize_and_format(examples):\n",
    "    contents = examples[\"content\"]\n",
    "    \n",
    "    # Truncate contents in batch\n",
    "    truncated_contents = []\n",
    "    for content in contents:\n",
    "        tokens = summarizer_tokenizer.encode(content, truncation=True)\n",
    "        if len(tokens) > 1000:\n",
    "            content = summarizer_tokenizer.decode(tokens[:1000])\n",
    "        truncated_contents.append(content)\n",
    "    \n",
    "    # Process contents in batch\n",
    "    processed_texts = []\n",
    "    summaries = summarizer(truncated_contents, max_length=64, min_length=24)\n",
    "    for i, content in enumerate(contents):\n",
    "        title = content[:200].split('.')[0] + '.'\n",
    "        summary = summaries[i]['summary_text']\n",
    "        processed_texts.append(f\"{title} - {summary}\")\n",
    "    \n",
    "    return {\"text\": processed_texts}\n",
    "\n",
    "# Extract sentiment from metrics column\n",
    "def extract_sentiment(example):\n",
    "    metrics = example[\"metrics\"]\n",
    "    positive_label = metrics[2][\"label\"] \n",
    "    negative_label = metrics[3][\"label\"]  \n",
    "    sentiment = positive_label + (negative_label * -1)    # 1 for positive, -1 for negative, 0 for neutral\n",
    "    mapping = {-1: 0, 0: -1, 1: 1}\n",
    "    \n",
    "    return {\"labels\": mapping[sentiment]}\n",
    "\n",
    "# Filter out neutral examples (label -1)\n",
    "def filter_neutral(example):\n",
    "    return int(example[\"labels\"]) != -1\n",
    "\n",
    "# Filter non-Bitcoin examples\n",
    "def filter_examples(example):\n",
    "    return example[\"metrics\"][0][\"label\"] != 0\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing functions to each split\n",
    "for split in [\"train\", \"test\"]:\n",
    "    dataset[split] = dataset[split].filter(filter_examples)\n",
    "    dataset[split] = dataset[split].map(extract_sentiment)\n",
    "    dataset[split] = dataset[split].filter(filter_neutral)\n",
    "    dataset[split] = dataset[split].map(summarize_and_format, batched=True, batch_size=32)\n",
    "    dataset[split] = dataset[split].map(tokenize_function, batched=True)\n",
    "    dataset[split].set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "dataset.save_to_disk(\"./models/processed_bitcoin_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06aeb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test into validation and test\n",
    "dataset = load_from_disk(\"./models/processed_bitcoin_sentiment\")\n",
    "test_val_split = dataset[\"test\"].train_test_split(test_size=0.5, seed=11)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = test_val_split[\"train\"]  \n",
    "test_dataset = test_val_split[\"test\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6ef59",
   "metadata": {},
   "source": [
    "#### Model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f6d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=finetuned_model_name,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"steps\", \n",
    "    eval_steps=100,  \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200, \n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.03, \n",
    "    logging_steps=100,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=4, \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Create early stopping callback\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=6,\n",
    "    early_stopping_threshold=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[early_stopping] \n",
    ")\n",
    "\n",
    "trainer.train() \n",
    "model.save_pretrained(finetuned_model_name)\n",
    "tokenizer.save_pretrained(finetuned_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13089102",
   "metadata": {},
   "source": [
    "#### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89a85f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def predict(dataset, classifier):\n",
    "    results = classifier(dataset[\"text\"], truncation=True, max_length=256, batch_size=16)\n",
    "    true_labels = dataset[\"labels\"]\n",
    "    label_map = {\"negative\": 0, \"positive\": 1}\n",
    "    pred_labels = [label_map[result[\"label\"]] for result in results]\n",
    "    \n",
    "    return true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c204867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models on evaluation dataset\n",
    "original_classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "true_labels, original_pred_labels = predict(dataset[\"test\"], original_classifier)\n",
    "\n",
    "finetuned_classifier = pipeline(\"sentiment-analysis\", model=finetuned_model_name, tokenizer=finetuned_model_name)\n",
    "true_labels, finetuned_pred_labels = predict(dataset[\"test\"], finetuned_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58c65d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model accuracy: 0.4599\n",
      "Finetuned model accuracy: 0.8368\n",
      "Original model report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.24      0.40      0.30       303\n",
      "    positive       0.66      0.48      0.56       745\n",
      "\n",
      "    accuracy                           0.46      1048\n",
      "   macro avg       0.45      0.44      0.43      1048\n",
      "weighted avg       0.54      0.46      0.49      1048\n",
      "\n",
      "Finetuned model report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.69      0.71       303\n",
      "    positive       0.88      0.90      0.89       745\n",
      "\n",
      "    accuracy                           0.84      1048\n",
      "   macro avg       0.80      0.79      0.80      1048\n",
      "weighted avg       0.83      0.84      0.84      1048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "original_accuracy = accuracy_score(true_labels, original_pred_labels)\n",
    "original_report = classification_report(true_labels, original_pred_labels, target_names=[\"negative\", \"positive\"])\n",
    "\n",
    "finetuned_accuracy = accuracy_score(true_labels, finetuned_pred_labels)\n",
    "finetuned_report = classification_report(true_labels, finetuned_pred_labels, target_names=[\"negative\", \"positive\"])\n",
    "\n",
    "print(f\"Original model accuracy: {original_accuracy:.4f}\")\n",
    "print(f\"Finetuned model accuracy: {finetuned_accuracy:.4f}\")\n",
    "print(f\"Original model report:\\n {original_report}\")\n",
    "print(f\"Finetuned model report:\\n {finetuned_report}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
